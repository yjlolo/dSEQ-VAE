diff --git a/scripts/benchmark/run_dsae.sh b/scripts/benchmark/run_dsae.sh
index fcaf8d2..758f912 100755
--- a/scripts/benchmark/run_dsae.sh
+++ b/scripts/benchmark/run_dsae.sh
@@ -2,23 +2,28 @@ python train.py -m \
     hydra/launcher=joblib \
     train.random_seed=831,277 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     train.random_seed=736,8 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     train.random_seed=172,828 \
     model.z_dim=8,16,32 \
-
-python train.py -m \
-    hydra/launcher=joblib \
-    train.random_seed=269,750 \
-    model.z_dim=8,16,32 \
-
-python train.py -m \
-    hydra/launcher=joblib \
-    train.random_seed=98,127 \
-    model.z_dim=8,16,32 \
\ No newline at end of file
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
\ No newline at end of file
diff --git a/scripts/benchmark/run_freeze.sh b/scripts/benchmark/run_freeze.sh
index 6b3052a..4397e43 100755
--- a/scripts/benchmark/run_freeze.sh
+++ b/scripts/benchmark/run_freeze.sh
@@ -3,27 +3,30 @@ python train.py -m \
     model=FreezeDsae \
     train.random_seed=831,277 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=FreezeDsae \
     train.random_seed=736,8 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=FreezeDsae \
     train.random_seed=172,828 \
     model.z_dim=8,16,32 \
-
-python train.py -m \
-    hydra/launcher=joblib \
-    model=FreezeDsae \
-    train.random_seed=269,750 \
-    model.z_dim=8,16,32 \
-
-python train.py -m \
-    hydra/launcher=joblib \
-    model=FreezeDsae \
-    train.random_seed=98,127 \
-    model.z_dim=8,16,32 \
\ No newline at end of file
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
\ No newline at end of file
diff --git a/scripts/benchmark/run_tsdsae.sh b/scripts/benchmark/run_tsdsae.sh
index 4934730..f836098 100755
--- a/scripts/benchmark/run_tsdsae.sh
+++ b/scripts/benchmark/run_tsdsae.sh
@@ -1,29 +1,65 @@
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=831,277 \
+    train.random_seed=831 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=736,8 \
+    train.random_seed=736 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=172,828 \
+    train.random_seed=172 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=269,750 \
+    train.random_seed=277 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=98,127 \
-    model.z_dim=8,16,32 \
\ No newline at end of file
+    train.random_seed=8 \
+    model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
+
+python train.py -m \
+    hydra/launcher=joblib \
+    model=TsDsae \
+    train.random_seed=828 \
+    model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
\ No newline at end of file
diff --git a/scripts/benchmark/run_tsdsae_woReg.sh b/scripts/benchmark/run_tsdsae_woReg.sh
index efddb7c..a32f070 100755
--- a/scripts/benchmark/run_tsdsae_woReg.sh
+++ b/scripts/benchmark/run_tsdsae_woReg.sh
@@ -1,34 +1,71 @@
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=831,277 \
+    train.random_seed=831 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
     model.reg_weights=[0,0,0,0] \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=736,8 \
+    train.random_seed=736 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
     model.reg_weights=[0,0,0,0] \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=172,828 \
+    train.random_seed=172 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
     model.reg_weights=[0,0,0,0] \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=269,750 \
+    train.random_seed=277 \
     model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
     model.reg_weights=[0,0,0,0] \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
 
 python train.py -m \
     hydra/launcher=joblib \
     model=TsDsae \
-    train.random_seed=98,127 \
+    train.random_seed=8 \
     model.z_dim=8,16,32 \
-    model.reg_weights=[0,0,0,0] \
\ No newline at end of file
+    optim.optimizer.amsgrad=True,False \
+    model.reg_weights=[0,0,0,0] \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
+
+python train.py -m \
+    hydra/launcher=joblib \
+    model=TsDsae \
+    train.random_seed=828 \
+    model.z_dim=8,16,32 \
+    optim.optimizer.amsgrad=True,False \
+    model.reg_weights=[0,0,0,0] \
+    logging.media_log.reconstruction=False \
+    logging.media_log.generation=False \
+    logging.media_log.z_swap=False \
+    logging.media_log.v_project=False \
\ No newline at end of file
diff --git a/src/models/base.py b/src/models/base.py
index 80949fa..76d832e 100644
--- a/src/models/base.py
+++ b/src/models/base.py
@@ -286,6 +286,9 @@ class DsaeBase(pl.LightningModule, ABC):
         train_out = outputs[0]
         outputs = outputs[1]
 
+        # Log everything on Wandb with the best ckpt
+        self._epoch_end(outputs, 'test')
+
         # Train LDA with outputs derived from the training set
         y_train = torch.cat([d_i['ground_truth'][:, -1] for d_i in train_out])
         x = torch.cat([d_i['v_mu'] for d_i in train_out])
@@ -296,7 +299,7 @@ class DsaeBase(pl.LightningModule, ABC):
         max_len = int(max(seq_lengths))
         labels = torch.cat([d_i['ground_truth'][:, -1] for d_i in outputs])
 
-        score, k = metric.evaluate_latent_swap(
+        score, _ = metric.evaluate_latent_swap(
             model=self,
             reconstruction=torch.cat(
                 [d_i['reconstruction'] for d_i in outputs]).to(self.device),
@@ -338,7 +341,7 @@ class DsaeBase(pl.LightningModule, ABC):
         print(output_dict)
         orig_cwd = Path(hydra.utils.get_original_cwd())
         fname = "_".join([f"{k}={v}" for k, v in output_dict['params'].items()])
-        fname = orig_cwd / 'scores' / f'{run_id}_{fname}.json'
+        fname = orig_cwd / 'benchmark' / f'{run_id}_{fname}.json'
         with open(fname, 'w', encoding='utf-8') as f:
             json.dump(output_dict, f, ensure_ascii=False, indent=4)
 
@@ -370,19 +373,19 @@ class DsaeBase(pl.LightningModule, ABC):
         # NOTE: put the index -1 to the label only for convenience (instrument)
         labels = torch.cat([d_i['ground_truth'][:, -1] for d_i in outputs])
 
-        if self.hparams.logging.media_log.reconstruction:
+        if self.hparams.logging.media_log.reconstruction or prefix == 'test':
             x_hat = outputs[0]['reconstruction'][0]
             x = outputs[0]['input'][0]
             self._log_reconstruction(x, x_hat, prefix)
 
-        if self.hparams.logging.media_log.v_project:
+        if self.hparams.logging.media_log.v_project or prefix == 'test':
             v = torch.cat([d_i['v'] for d_i in outputs])
             self._log_v_projection(v, labels, prefix)
 
-        if self.hparams.logging.media_log.generation:
+        if self.hparams.logging.media_log.generation or prefix == 'test':
             self._log_generation(1, max_len, prefix)
         
-        if self.hparams.logging.media_log.z_swap:
+        if self.hparams.logging.media_log.z_swap or prefix == 'test':
             v = torch.cat([d_i['v'] for d_i in outputs])
             z_pos = pad_batch(outputs, 'z_pos', max_len)
             try:
